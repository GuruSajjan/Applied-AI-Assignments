{"cells":[{"cell_type":"markdown","metadata":{"id":"5HExLQrE4ZxR"},"source":["<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"]},{"cell_type":"markdown","metadata":{"id":"4LuKrFzC4ZxV"},"source":["<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"1wES-wWN4ZxX"},"source":["<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n","\n","Check the documentation for better understanding of these attributes: \n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","<img src='https://i.imgur.com/K11msU4.png' width=500>\n","\n","As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n","\n","Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n","\n","Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n","\n","Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n","$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n","\n","RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n","\n","For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"z830CfMk4Zxa"},"source":["## Task E"]},{"cell_type":"markdown","metadata":{"id":"MuBxHiCQ4Zxc"},"source":["> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n","\n","> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n","\n","> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"fCgMNEvI4Zxf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"ANUNIqCe4Zxn"},"outputs":[],"source":["X, Y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n","                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3000, 5) (1000, 5) (1000, 5)\n","(3000,) (1000,) (1000,)\n"]}],"source":["Y[Y==0] = -1\n","X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n","X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=int(len(X)*0.2))\n","print(X_train.shape,X_val.shape,X_test.shape)\n","print(y_train.shape,y_val.shape,y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"tHie1zqH4Zxt"},"source":["### Pseudo code\n","\n","clf = SVC(gamma=0.001, C=100.)<br>\n","clf.fit(Xtrain, ytrain)\n","\n","<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n","   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n","    \n","fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n","\n","<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["3000"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["len(X_train)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True]\n","559\n","[0.39829954]\n"]}],"source":["clf = SVC(C = 100,kernel='rbf',gamma=0.001)\n","clf.fit(X_train,y_train)\n","print(clf.dual_coef_[0]>0)\n","print(len(clf.support_vectors_))\n","print(clf.intercept_)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["def kernal_val(x_q,sup_vect,gamma):\n","    return np.exp(-1 * gamma * np.linalg.norm(sup_vect-x_q,axis = 1))"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"h43kDT3M41u5"},"outputs":[],"source":["def decision_function(X_val,coef,sup_x,sup_y,intercept,gamma = 0.001):\n","    decision_val = []\n","    for x_q in X_val:\n","        K = kernal_val(x_q,sup_x,gamma) \n","        decision_val.append(np.sum(sup_y*coef*K)+intercept)\n","    return np.array(decision_val)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[55674.38861367 55510.54777545 55666.50193279 55625.70902711\n"," 55670.54214006 55685.02208387 55690.95399305 55689.97134955\n"," 55696.62280717 55676.90560468 55641.5827374  55613.66822946\n"," 55672.23637215 55686.87478638 55700.78995258 55659.57752475\n"," 55644.18432922 55648.53294472 55666.84449368 55654.9688771\n"," 55655.04770389 55689.40670051 55697.71386456 55703.65036513\n"," 55672.18383633 55712.07830337 55636.85446703 55671.35289962\n"," 55645.70454356 55633.40840764 55663.24667527 55697.34288343\n"," 55670.23019612 55652.13539186 55658.32630693 55653.68503718\n"," 55645.8454064  55656.74313206 55685.60722776 55698.45535451\n"," 55664.80329191 55664.37656103 55672.86118703 55643.93690309\n"," 55668.16381304 55658.12877283 55667.82162895 55710.15106454\n"," 55646.7816816  55680.14056856 55664.91842549 55685.25894571\n"," 55654.20989259 55712.24113691 55695.04628014 55665.7004665\n"," 55672.08075079 55679.70140598 55661.65441782 55630.01724456\n"," 55613.81846094 55683.50515677 55650.25830924 55674.90706994\n"," 55690.28053739 55672.94452733 55667.01285636 55604.24615798\n"," 55642.97292502 55670.58885025 55677.52069856 55674.03197512\n"," 55693.63144774 55682.43824496 55672.25061215 55670.59045537\n"," 55649.51240127 55667.78639535 55669.37243493 55701.47573264\n"," 55709.92968402 55666.64633688 55660.67223561 55661.15854987\n"," 55620.25923255 55584.49376891 55620.38653923 55659.94455112\n"," 55673.56647999 55671.03390034 55694.123355   55675.01517833\n"," 55581.07325632 55640.4140454  55681.72217405 55645.72425796\n"," 55656.69994584 55657.46242768 55629.7641642  55683.02939187\n"," 55646.55673482 55690.0407994  55687.69786799 55672.14049726\n"," 55655.75265406 55686.85757309 55693.84010915 55681.32250085\n"," 55690.23955824 55634.65379484 55639.92056433 55620.1616575\n"," 55669.68760174 55673.87658886 55661.51867988 55687.24885211\n"," 55659.43960513 55667.84002314 55686.77352422 55620.70186127\n"," 55665.29353805 55679.99435113 55709.02808617 55654.43677601\n"," 55683.40878857 55646.83518545 55661.0469359  55684.60550158\n"," 55665.33412426 55684.87423212 55640.48377825 55670.04288557\n"," 55683.43959554 55684.27432436 55643.45987885 55660.06233571\n"," 55701.25806166 55663.52781008 55700.53216448 55674.1769821\n"," 55687.50859716 55559.2653678  55658.79216934 55610.61378198\n"," 55674.45779391 55660.58329231 55653.97116456 55702.24803793\n"," 55666.54257906 55671.6163693  55655.55038298 55651.10140465\n"," 55673.04819202 55688.80168345 55643.01566674 55627.82015315\n"," 55671.00868393 55666.98932402 55695.58108149 55683.50298995\n"," 55675.19445861 55612.43628306 55635.65651748 55665.80069666\n"," 55662.7685036  55642.52771809 55662.82784542 55660.53776971\n"," 55615.24430822 55671.68127131 55667.42491643 55657.66112239\n"," 55641.27444992 55643.10494322 55615.77038124 55644.48449056\n"," 55627.70631317 55670.28965806 55636.3212106  55650.44948081\n"," 55613.38333794 55704.70573508 55678.54701109 55637.91393051\n"," 55680.95414558 55660.76339653 55666.44102684 55674.60548764\n"," 55707.7470606  55647.24481855 55685.78651266 55607.80637356\n"," 55669.97440846 55689.36826374 55674.72669687 55685.27057926\n"," 55627.31194472 55628.90531399 55660.48529258 55712.70674036\n"," 55675.31558985 55634.53453638 55655.16245547 55712.06568855\n"," 55610.43478905 55711.45125765 55639.68982636 55642.93443677\n"," 55673.04517421 55655.99891568 55670.74050501 55648.58718113\n"," 55658.24424876 55638.12979387 55695.85525737 55687.1755884\n"," 55680.19757877 55646.80215325 55687.71261785 55599.65750569\n"," 55662.55383006 55671.79083713 55683.19912415 55682.76099604\n"," 55590.71317461 55673.09790671 55678.40403133 55659.06156959\n"," 55672.48014034 55627.60332806 55668.81399557 55661.29238394\n"," 55703.44757368 55710.67044532 55658.36804995 55649.64421384\n"," 55679.76611046 55652.02765624 55649.34530253 55551.7555669\n"," 55611.8837439  55673.83877154 55684.29751141 55687.62546855\n"," 55648.25138928 55680.01377939 55660.27205146 55663.43022483\n"," 55570.59299595 55655.95528631 55684.87008064 55637.25295182\n"," 55682.87185618 55697.30568181 55675.55441762 55670.18134696\n"," 55661.90569335 55679.11792947 55676.11761419 55605.28409439\n"," 55690.57987068 55669.5527279  55648.33359291 55648.24195086\n"," 55661.01963305 55707.56850377 55662.03883027 55699.5683415\n"," 55649.95587548 55673.87181314 55637.69608533 55669.8077296\n"," 55656.87652779 55679.00110884 55670.99216635 55706.59399678\n"," 55705.19859597 55665.33238365 55648.11115926 55701.59103852\n"," 55663.28799524 55688.3315219  55661.49757003 55645.36661578\n"," 55653.1801952  55699.130165   55701.55482801 55658.83983366\n"," 55631.3552252  55656.41848578 55684.57923604 55669.67474515\n"," 55663.15636605 55657.27019248 55680.50842472 55668.90337673\n"," 55615.67632958 55702.8053769  55677.32682578 55638.45621172\n"," 55703.20198726 55684.67115515 55672.19619174 55660.32650975\n"," 55678.95048783 55696.50230784 55653.66978656 55649.00140641\n"," 55670.9154531  55626.43385002 55639.04957253 55662.84282602\n"," 55657.67153794 55614.43707619 55671.45897084 55645.28994799\n"," 55641.92697163 55691.85204463 55695.3950113  55617.87590961\n"," 55672.7544516  55649.08965943 55668.14335004 55677.5666045\n"," 55640.58149556 55607.11512469 55698.84095319 55654.11355444\n"," 55679.30872407 55633.84202094 55666.64780157 55636.1452283\n"," 55644.2862873  55589.35153821 55619.97383585 55580.30737138\n"," 55686.98602884 55633.48185593 55679.69633644 55665.58497697\n"," 55675.17982827 55674.14408359 55676.98148051 55695.10388992\n"," 55620.97463388 55654.79508351 55675.0787597  55689.10477317\n"," 55571.49356098 55656.56075975 55684.2765775  55630.40147667\n"," 55693.95205517 55659.80292181 55677.7539276  55663.05884747\n"," 55660.28917098 55667.70985951 55645.07624507 55639.86271635\n"," 55622.26543655 55681.87916175 55667.45378354 55684.21236388\n"," 55597.74601603 55664.87293905 55665.49163596 55684.76941776\n"," 55686.39105995 55658.35646167 55639.51325828 55590.64872327\n"," 55660.34034028 55690.66516219 55656.26612691 55667.88588467\n"," 55683.71080988 55695.20839936 55647.1164141  55681.45660696\n"," 55687.84185829 55669.90725007 55695.98959117 55688.19938938\n"," 55659.31981723 55634.56084274 55679.269824   55685.65399453\n"," 55665.29092926 55650.2197677  55637.32924319 55685.67141012\n"," 55659.29700038 55674.61222914 55677.81186804 55660.61519642\n"," 55666.40947667 55653.01248321 55655.58485459 55646.36773737\n"," 55706.25908635 55637.26762036 55694.55570732 55705.99233548\n"," 55663.13306683 55653.54059282 55660.96796357 55665.3039617\n"," 55577.79092854 55664.00927633 55645.09406344 55662.99393565\n"," 55683.06635557 55682.92015154 55666.25683683 55649.86707721\n"," 55662.30458596 55649.07278641 55696.42897665 55710.88735851\n"," 55624.84717457 55674.01028857 55684.48874701 55692.2501847\n"," 55657.46689661 55663.39402432 55699.74322299 55689.90423379\n"," 55690.25725299 55648.79938027 55637.60923553 55674.51570717\n"," 55682.54078857 55688.22212695 55712.47689167 55633.76744834\n"," 55704.26132842 55666.53449077 55658.69737274 55678.42828181\n"," 55677.62967506 55674.00578318 55689.8650191  55678.23390708\n"," 55636.5353514  55692.00107691 55638.89124312 55656.4202925\n"," 55685.39276152 55675.55641219 55700.02515126 55669.42732212\n"," 55635.66647151 55675.30138176 55694.88384873 55610.37867586\n"," 55638.40889453 55659.40994495 55658.21490224 55676.20733793\n"," 55671.02847852 55665.36281915 55671.3888944  55641.20576125\n"," 55697.5057131  55693.63472934 55700.98638345 55668.21377032\n"," 55673.61857706 55634.7732491  55640.88312312 55688.1812639\n"," 55708.23276717 55672.39938932 55679.54839806 55658.3979771\n"," 55636.21345526 55698.05216633 55671.96029539 55672.6802429\n"," 55680.72034535 55676.13830384 55634.69359739 55642.6529365\n"," 55690.50604205 55692.22957512 55684.88393204 55653.75828032\n"," 55632.32804174 55633.1849406  55593.57564811 55673.41633541\n"," 55633.1648435  55658.8074751  55667.35041235 55638.10930432\n"," 55693.86733283 55660.72650801 55671.02849083 55635.53933805\n"," 55637.74578845 55660.99423644 55708.90752656 55629.62313302\n"," 55665.90319137 55651.6909525  55656.79868892 55639.73086948\n"," 55686.30224006 55686.91874913 55693.32884993 55680.87538679\n"," 55698.43640678 55681.18526791 55688.8234643  55581.18151136\n"," 55697.12352703 55658.34467147 55693.86573606 55692.02432094\n"," 55543.74146665 55680.80763773 55679.76782864 55658.93937667\n"," 55627.76628397 55676.7888768  55676.01807321 55698.85897098\n"," 55644.58679442 55657.13867094 55674.50028377 55653.01784843\n"," 55700.33872808 55685.76655198 55643.78240297 55673.31759004\n"," 55679.68739372 55670.64944505 55679.78878642 55657.68375104\n"," 55652.68523873 55632.1361434  55691.65268725 55666.25576639\n"," 55613.98464342 55651.78657813 55671.64237447 55605.80967759\n"," 55676.01751545 55695.06116147 55677.51491762 55521.72946808\n"," 55687.64758166 55690.34199033 55668.37513852 55664.52189015\n"," 55670.92573861 55654.28442833 55678.3781871  55689.05163768\n"," 55703.43933282 55666.99789333 55664.55467694 55658.56275517\n"," 55661.92228738 55701.07326546 55690.02464619 55689.8650292\n"," 55672.85267601 55683.71155496 55646.71876093 55542.24693038\n"," 55671.28167894 55665.76046723 55662.14673616 55622.50148095\n"," 55670.14485474 55622.57351374 55555.764678   55679.29073152\n"," 55656.39307558 55671.97420092 55685.79130455 55660.77946876\n"," 55645.88993819 55653.8142084  55658.55624229 55644.5135607\n"," 55579.77958383 55671.57635152 55666.37438221 55647.04502578\n"," 55661.31307433 55680.8700612  55654.16728301 55694.16434263\n"," 55638.27312504 55634.73965018 55669.667657   55688.41015645\n"," 55664.83826033 55671.44277476 55640.1987349  55644.77456595\n"," 55712.64610446 55676.38484272 55619.68707005 55665.93703942\n"," 55633.91269087 55696.69238497 55646.21332402 55589.41998153\n"," 55641.76315766 55651.75849801 55650.43732528 55693.26432199\n"," 55692.24570238 55682.95428931 55683.91438137 55680.14262723\n"," 55657.46157317 55645.01887326 55675.81634051 55646.45217125\n"," 55668.97498084 55668.40377883 55640.85842902 55712.57322052\n"," 55635.2541142  55672.50301702 55662.85772987 55672.10654856\n"," 55657.5474256  55670.73073578 55668.7912089  55679.59559981\n"," 55548.80754112 55643.28744025 55655.34073091 55668.45101983\n"," 55676.02293285 55673.36082063 55698.30424815 55632.18585456\n"," 55690.25383017 55681.42418224 55695.56132249 55630.4155442\n"," 55696.73960799 55650.86760208 55641.36035138 55680.24051671\n"," 55682.19389312 55670.2079151  55658.47953882 55660.97416762\n"," 55672.68215889 55676.51476753 55662.42973812 55664.63313842\n"," 55676.52814787 55690.21635037 55619.561509   55710.98616505\n"," 55689.42349897 55692.08549277 55552.20228844 55623.46214977\n"," 55625.85315912 55624.97181102 55656.66120822 55685.39771217\n"," 55608.0968637  55663.91446056 55617.84674921 55590.17614206\n"," 55673.11374042 55596.4847564  55661.31982259 55701.24989869\n"," 55643.02363704 55627.46474207 55673.01823588 55574.8269589\n"," 55533.30830242 55705.88554972 55687.58941362 55702.60408562\n"," 55675.52964818 55665.45332861 55651.89922793 55582.59874372\n"," 55682.4280182  55666.21111838 55488.01367072 55666.44224506\n"," 55681.45804432 55661.25536538 55693.57870679 55665.26200647\n"," 55607.9993213  55696.92837939 55666.74221928 55681.68894064\n"," 55635.19757975 55682.76629657 55677.02750512 55654.15995796\n"," 55695.20329256 55629.27327329 55653.47719404 55675.67448519\n"," 55652.27044596 55670.38685214 55693.99931282 55687.54550284\n"," 55659.5950461  55663.61712393 55556.37770173 55515.4515039\n"," 55665.95904381 55700.20788303 55636.72903656 55698.07467855\n"," 55666.91043981 55647.91976243 55664.23034026 55674.16250699\n"," 55624.89722383 55680.93497309 55688.196457   55628.70611152\n"," 55643.20327722 55707.62041269 55697.67505513 55663.78209862\n"," 55675.03685984 55662.43604405 55670.89978133 55669.08072651\n"," 55585.42359838 55669.80352999 55681.49450378 55659.07597048\n"," 55657.21917801 55589.01125283 55712.2962036  55649.31730061\n"," 55662.20743513 55682.80420276 55670.70089675 55689.62118058\n"," 55663.15816726 55603.72768982 55666.86107255 55697.24876771\n"," 55667.68950397 55621.95919491 55695.75037749 55676.02694002\n"," 55702.4074998  55686.65667519 55695.56382993 55663.2457161\n"," 55683.05417455 55692.55309598 55671.70998524 55673.5333996\n"," 55640.3651182  55679.93535316 55622.65266642 55636.34949649\n"," 55634.60626647 55701.87217101 55687.06511053 55632.92710399\n"," 55694.01474531 55701.68544778 55678.22321668 55620.63681919\n"," 55653.81434843 55677.84494989 55684.75531206 55602.36583409\n"," 55669.60794431 55696.85110757 55683.60746973 55648.11688182\n"," 55653.86633541 55672.00755724 55679.32003289 55558.27375751\n"," 55671.56006026 55608.95240159 55639.95024956 55576.41618236\n"," 55652.04608687 55655.4683939  55681.48384136 55564.49791452\n"," 55685.36489011 55664.83081478 55684.58075955 55638.69237984\n"," 55659.78205774 55667.83563309 55645.15481755 55674.27870195\n"," 55680.54619539 55663.40449174 55691.98416998 55678.5426976\n"," 55692.20767533 55632.17149758 55645.83448557 55677.49820322\n"," 55686.50203357 55626.13344641 55686.68615793 55693.74858138\n"," 55674.5623284  55645.77625451 55689.94581579 55671.21463486\n"," 55662.20701697 55682.35595584 55663.07128898 55629.77299224\n"," 55612.62536287 55694.20990417 55699.74855967 55618.50997997\n"," 55707.28118651 55582.20607351 55688.50685363 55678.81864069\n"," 55672.3433867  55640.31910162 55682.52883388 55680.69061181\n"," 55635.35449764 55674.804243   55655.47690377 55658.38445562\n"," 55669.96737139 55697.26072794 55666.80836094 55696.80758882\n"," 55673.91989746 55640.39652674 55666.47420681 55686.51174847\n"," 55661.41510139 55678.43210134 55678.52891214 55619.08829728\n"," 55681.39090491 55663.29373441 55642.60228231 55708.81824083\n"," 55634.71945404 55682.27569109 55652.95033289 55685.00720253\n"," 55647.98670561 55675.204194   55665.340185   55696.83255861\n"," 55603.98000919 55681.1099986  55679.17930719 55552.98680149\n"," 55666.3252151  55713.44426209 55681.79354903 55675.75623396\n"," 55708.57899407 55672.90686036 55698.89594395 55703.45878662\n"," 55580.58718321 55670.44187353 55618.81439526 55658.68807226\n"," 55702.30071934 55637.70956504 55623.86845294 55650.2456126\n"," 55658.79194563 55643.90248457 55632.29756628 55685.72312754\n"," 55652.98534626 55651.17461407 55634.9815881  55677.68727757\n"," 55676.34468253 55617.65212912 55663.99314817 55649.26216212\n"," 55639.8843242  55610.43114844 55671.44335217 55674.91707775\n"," 55669.65600831 55597.93970719 55708.19315463 55687.33506024\n"," 55651.22852374 55590.56605665 55630.48176739 55693.22494796\n"," 55627.0295447  55633.05163536 55652.83745689 55710.4735081\n"," 55679.65192682 55678.22117647 55656.10421198 55666.8095022\n"," 55641.86868934 55683.92328127 55688.61204833 55672.92239129\n"," 55635.70015016 55679.29426941 55655.00445328 55691.65374229\n"," 55646.31821695 55672.96074441 55682.44229593 55641.07273637\n"," 55628.55388522 55666.5714422  55669.62284793 55602.36658206\n"," 55684.16543675 55690.67498041 55687.24982831 55670.30885231\n"," 55624.75426742 55668.05431215 55646.21040241 55675.01898278\n"," 55684.27913654 55664.18669538 55635.38236992 55675.80024091\n"," 55693.89587522 55641.79914432 55653.11290653 55693.78931488\n"," 55624.65315748 55639.53043709 55612.24849364 55678.30506958\n"," 55591.86490169 55607.80978614 55693.35336383 55625.92221641\n"," 55692.19140399 55662.86880399 55614.56718975 55681.37356899\n"," 55660.03445703 55685.9571083  55590.48772675 55638.81302617\n"," 55640.8753234  55689.65728251 55684.2738794  55663.12438722\n"," 55680.63287401 55673.84353491 55689.25748644 55651.36705823\n"," 55670.91693773 55635.93690184 55679.07153603 55658.71480486\n"," 55667.18009167 55645.40432322 55708.17067105 55661.12079588\n"," 55670.4680224  55658.30542505 55695.97385028 55676.59306405\n"," 55635.51045709 55628.79043413 55694.72301237 55668.75586342\n"," 55662.78697416 55668.31740451 55675.55615781 55701.79194332\n"," 55656.74180374 55685.51041997 55665.1645408  55652.65587051]\n","\n","\n","\n"]}],"source":["Y_our_pred = decision_function(X_val,clf.dual_coef_[0],clf.support_vectors_,y_train[clf.support_],clf.intercept_[0],0.001)\n","Y_pred = clf.decision_function(X_val)\n","print(Y_our_pred - Y_pred)\n","print(\"\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"c0bKCboN4Zxu"},"source":["<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"nMn7OEN94Zxw"},"source":["Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n","<img src='https://i.imgur.com/CAMnVnh.png'>\n"]},{"cell_type":"markdown","metadata":{"id":"e0n5EFkx4Zxz"},"source":["## TASK F"]},{"cell_type":"markdown","metadata":{"id":"t0HOqVJq4Zx1"},"source":["\n","> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n","\n","> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n","\n","> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n","<img src='https://i.imgur.com/zKYE9Oc.png'>\n","if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n","\n","> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"]},{"cell_type":"markdown","metadata":{"id":"oTY7z2bd4Zx2"},"source":["__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"]},{"cell_type":"markdown","metadata":{"id":"CM3odN1Z4Zx3"},"source":["\n","If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n","\n","1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n","\n","2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n","\n","3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n","\n","4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"8E&F_LR_SVM.ipynb","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"156a3ee15799716d58d55a87e405c1165f4d4c7e5471a2b0ac309addf8a51cbe"}}},"nbformat":4,"nbformat_minor":0}
