{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11f61412-9d1f-468b-a319-b06d0c097153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f11df78-e777-4b41-be7c-e7e7c93b8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it accepts only list of sentances\n",
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    idf = [] # idf value of each unique word\n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        # calculates the IDF for each word in vocabulary\n",
    "        for word in vocab.keys():\n",
    "            count = 0\n",
    "            for doc in dataset:\n",
    "                if word in doc.split(\" \"):\n",
    "                    count+=1\n",
    "            k = math.log((len(dataset)+1)/(count+1))+1        \n",
    "            idf.append(k)        \n",
    "        return vocab,idf    \n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24ee8e59-7270-423a-82cc-883d539ff40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(corpus,vocabulary,IDF):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    TF = []\n",
    "    values = []\n",
    "    \n",
    "    #Here we create word frequency matrix and total number of words in document of Corpus\n",
    "    if(isinstance(corpus,list)):\n",
    "        for row,doc in enumerate(corpus):\n",
    "            word_freq = dict(Counter(doc.split(\" \")))\n",
    "            total_words = 0\n",
    "            for word,freq in word_freq.items():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col = vocabulary.get(word,-1)\n",
    "                total_words = total_words + freq\n",
    "                if col != -1 :\n",
    "                    rows.append(row)\n",
    "                    columns.append(col)\n",
    "                    values.append(freq)\n",
    "            TF.append(total_words)        \n",
    "    Freq_Matrix = csr_matrix((values,(rows,columns)),(len(corpus),len(vocabulary)),float).toarray()\n",
    "\n",
    "    #store the TF of each word in document\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(vocabulary)): \n",
    "            if TF[i] != 0:\n",
    "                Freq_Matrix[i][j] = Freq_Matrix[i][j]/TF[i]\n",
    "            else:\n",
    "                Freq_Matrix[i][j] = 0\n",
    "\n",
    "\n",
    "    # store TF-IDF value of each cell in Matrix        \n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            if(Freq_Matrix[i][j] > 0):\n",
    "                Freq_Matrix[i][j] = IDF[j] * Freq_Matrix[i][j]\n",
    "\n",
    "    Freq_Matrix = normalize(Freq_Matrix,norm = \"l2\")                       \n",
    "\n",
    "    return Freq_Matrix                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e3b774a-da9f-49f2-810a-d3c0a68a465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense_matrix(Matrix,k):\n",
    "     doc = dict()\n",
    "     for i in range(len(Matrix[k])):\n",
    "         doc[(k,i)] = Matrix[k][i]\n",
    "\n",
    "     # Sort the dictionary based on value(tf-idf value) in non-increasing order\n",
    "     result = dict(sorted(doc.items(),key = lambda item:item[0],reverse = True ))    \n",
    "\n",
    "     # print only non-zero values and corresponding index in matrix\n",
    "     for key,value in result.items():\n",
    "         if(value > 0):\n",
    "             print(\"{} : {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91e759f1-e642-4f81-babc-6759139713ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all Unique words in Corpus :  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] \n",
      "\n",
      "List of IDF values of each word :  [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0] \n",
      "\n",
      "Shape of Matrix :  (4, 9) \n",
      "\n",
      "[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      " 0.38408524 0.         0.38408524] \n",
      "\n",
      "(3, 8) : 0.3840852409148149\n",
      "(3, 6) : 0.3840852409148149\n",
      "(3, 3) : 0.3840852409148149\n",
      "(3, 2) : 0.580285823684436\n",
      "(3, 1) : 0.4697913855799205\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "vocab,idf = fit(corpus)\n",
    "s_matrix = transform(corpus,vocab,idf)\n",
    "print(\"List of all Unique words in Corpus : \",list(vocab.keys()),\"\\n\")\n",
    "\n",
    "print(\"List of IDF values of each word : \",idf,\"\\n\")\n",
    "\n",
    "print(\"Shape of Matrix : \",s_matrix.shape,\"\\n\")\n",
    "print(s_matrix[3],\"\\n\")\n",
    "print(Dense_matrix(s_matrix,3),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b254c-def2-4951-8196-b03832716df8",
   "metadata": {},
   "source": [
    "## SKlearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8aa517c-541a-4edd-a8ba-1541deebba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)\n",
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape\n",
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "print(skl_output[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488c196-41a3-4bd1-9e3e-fe1407856e2d",
   "metadata": {},
   "source": [
    "## Task - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "525c00c4-b1af-437d-a394-36d70f68afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit50(corpus):\n",
    "    unique_words = set() # This SET will contain all unique words of corpus\n",
    "    idf = [] # list containing top 50 idf values\n",
    "    #If corpus is list ,Add words to set where len is greter than 1\n",
    "    if(isinstance(corpus,list)):\n",
    "        for doc in corpus:\n",
    "            for word in doc.split(\" \"):\n",
    "                if len(word)<2 :\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        # Sort it, With list then enumate to create dictionary to give value to each unique word of corpus\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocabulary = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        #count occurance of each word in corpus and sort them in increasing order\n",
    "        count_words = dict()\n",
    "        if(isinstance(corpus,list)):\n",
    "            for word in vocabulary:\n",
    "                count = 0\n",
    "                for doc in corpus:\n",
    "                    if word in doc.split():\n",
    "                        count+=1\n",
    "                count_words[word] = count\n",
    "            count_words = sorted(count_words.items(),key = lambda item:item[1],reverse = False )\n",
    "            \n",
    "        # create dict with top 50 less freq words    \n",
    "        new_vocabulary_count = dict()\n",
    "        count = 0\n",
    "        for i,j in count_words:\n",
    "            if count < 50:\n",
    "                new_vocabulary_count.update({i:j})\n",
    "                count+=1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        unique_words = sorted(list(new_vocabulary_count.keys()))\n",
    "        new_vocabulary = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        # calculate idf values for words in new_vocab and store in idf list\n",
    "        for word in new_vocabulary.keys():\n",
    "            count = 0\n",
    "            for doc in corpus:\n",
    "                if word in doc.split(\" \"):\n",
    "                    count+=1\n",
    "            k = math.log((len(corpus)+1)/(count+1))+1        \n",
    "            idf.append(k)\n",
    "            \n",
    "        return new_vocabulary,idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e247a-6ab1-4a31-84ed-0cdeba8d2b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2b0989b-f0b8-4f30-b146-1660ee58bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n",
      "List of top 50 idf value Unique words in Corpus :  ['aailiyah', 'abandoned', 'abroad', 'abstruse', 'academy', 'accents', 'accessible', 'acclaimed', 'accolades', 'accurate', 'accurately', 'achille', 'ackerman', 'actions', 'adams', 'add', 'added', 'admins', 'admiration', 'admitted', 'adrift', 'adventure', 'aesthetically', 'affected', 'affleck', 'afternoon', 'aged', 'ages', 'agree', 'agreed', 'aimless', 'aired', 'akasha', 'akin', 'alert', 'alike', 'allison', 'allow', 'allowing', 'alongside', 'amateurish', 'amaze', 'amazed', 'amazingly', 'amusing', 'amust', 'anatomist', 'angel', 'angela', 'angelina'] \n",
      "\n",
      "List of IDF values of each word :  [6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872] \n",
      "\n",
      "Shape of Matrix :  (746, 50) \n",
      "\n",
      "slow moving aimless movie distressed drifting young man \n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.] \n",
      "\n",
      "(0, 30) : 1.0\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))\n",
    "\n",
    "vocab,idf = fit50(corpus)\n",
    "s_matrix = transform(corpus,vocab,idf)\n",
    "print(\"List of top 50 idf value Unique words in Corpus : \",list(vocab.keys()),\"\\n\")\n",
    "\n",
    "print(\"List of IDF values of each word : \",idf,\"\\n\")\n",
    "\n",
    "print(\"Shape of Matrix : \",s_matrix.shape,\"\\n\")\n",
    "print(corpus[0],\"\\n\")\n",
    "print(s_matrix[0],\"\\n\")\n",
    "print(Dense_matrix(s_matrix,0),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374429a-6e77-4a80-a0da-f19be9d6ed85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
