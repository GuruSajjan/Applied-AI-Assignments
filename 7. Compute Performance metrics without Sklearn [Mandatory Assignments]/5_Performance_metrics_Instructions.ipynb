{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "\n",
    "## A. Compute performance metrics for the given data '5_a.csv'\n",
    " <pre>  <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y,y_hat):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for y_i,y_hat_i in zip(y,y_hat):\n",
    "        if y_i == y_hat_i:\n",
    "            if y_hat_i == 1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "        else:\n",
    "            if y_i == 1 and y_hat_i == 0:\n",
    "                FN+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    return [TN,FN,FP,TP]                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  1.0  0.637387\n",
      "1  1.0  0.635165\n",
      "2  1.0  0.766586\n",
      "3  1.0  0.724564\n",
      "4  1.0  0.889199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    10000\n",
       "0.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a=pd.read_csv('5_a.csv')\n",
    "df_a.head()\n",
    "print(df_a.head())\n",
    "df_a['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 100, 10000]\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.select([df_a['proba'] < 0.5,df_a['proba'] >= 0.5],[0,1])\n",
    "confusion_matrix = conf_matrix(list(df_a['y']),list(y_hat))\n",
    "TN = confusion_matrix[0]\n",
    "FN = confusion_matrix[1]\n",
    "FP = confusion_matrix[2]\n",
    "TP = confusion_matrix[3]\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Precision = TP/(FP + TP). It means that out of all the points our model predicted to be\n",
    "positive, what percentage of them are actually positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yg8uUJvGAfCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "precision = TP/(FP+TP)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall = TPR = TP/(FN + TP). It means that out of all positive points that were present\n",
    "in the dataset, how many of them were predicted successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "recall = TP/(TP+FN)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* F1-Score is Harmonic mean of precision and recall.\n",
    "* F1-Score = 2*(pricision*recall) / (pricision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*(precision*recall)/(precision+recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy can be defined as the ratio of the correctly classified and the total number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "accuracy = TP/(sum(confusion_matrix))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48829900000000004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = sorted(list(df_a['proba']),reverse=True)\n",
    "TPR = []\n",
    "FPR = []\n",
    "\n",
    "for thr in threshold:\n",
    "    y_hat = np.select([df_a['proba'] < thr,df_a['proba'] >= thr],[0,1])\n",
    "    confusion_matrix = conf_matrix(list(df_a['y']),list(y_hat))\n",
    "    TN = confusion_matrix[0]\n",
    "    FN = confusion_matrix[1]\n",
    "    FP = confusion_matrix[2]\n",
    "    TP = confusion_matrix[3]\n",
    "    tpr = TP/(TP+FN)\n",
    "    fpr = FP/(TN+FP)\n",
    "    TPR.append(tpr)\n",
    "    FPR.append(fpr)\n",
    "\n",
    "np.trapz(TPR,FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "\n",
    "\n",
    "## B. Compute performance metrics for the given data '5_b.csv'\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a>\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  0.0  0.281035\n",
      "1  0.0  0.465152\n",
      "2  0.0  0.352793\n",
      "3  0.0  0.157818\n",
      "4  0.0  0.276648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    10000\n",
       "1.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b=pd.read_csv('5_b.csv')\n",
    "print(df_b.head())\n",
    "df_b['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[9761, 45, 239, 55]\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.select([df_b['proba'] < 0.5,df_b['proba'] >= 0.5],[0,1])\n",
    "print(y_hat)\n",
    "confusion_matrix = conf_matrix(list(df_b['y']),list(y_hat))\n",
    "TN = confusion_matrix[0]\n",
    "FN = confusion_matrix[1]\n",
    "FP = confusion_matrix[2]\n",
    "TP = confusion_matrix[3]\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1870748299319728\n"
     ]
    }
   ],
   "source": [
    "precision = TP/(FP+TP)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "recall = TP/(TP+FN)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*(precision*recall)/(precision+recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005445544554455445\n"
     ]
    }
   ],
   "source": [
    "accuracy = TP/(sum(confusion_matrix))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377570000000001"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = sorted(list(df_b['proba']),reverse=True)\n",
    "TPR = []\n",
    "FPR = []\n",
    "\n",
    "for thr in threshold:\n",
    "    y_hat = np.select([df_b['proba'] < thr,df_b['proba'] >= thr],[0,1])\n",
    "    confusion_matrix = conf_matrix(list(df_b['y']),list(y_hat))\n",
    "    TN = confusion_matrix[0]\n",
    "    FN = confusion_matrix[1]\n",
    "    FP = confusion_matrix[2]\n",
    "    TP = confusion_matrix[3]\n",
    "    tpr = TP/(TP+FN)\n",
    "    fpr = FP/(TN+FP)\n",
    "    TPR.append(tpr)\n",
    "    FPR.append(fpr)\n",
    "\n",
    "np.trapz(TPR,FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "### C. Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data \n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y      prob\n",
      "0  0  0.458521\n",
      "1  0  0.505037\n",
      "2  0  0.418652\n",
      "3  0  0.412057\n",
      "4  0  0.375579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "1    1047\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c=pd.read_csv('5_c.csv')\n",
    "print(df_c.head())\n",
    "df_c['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "eAPjewjzAfCa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2300390278970873 141000\n"
     ]
    }
   ],
   "source": [
    "threshold = sorted(list(df_c['prob']),reverse=True)\n",
    "min_A = float(\"inf\")\n",
    "for thr in threshold:\n",
    "    y_hat = np.select([df_c['prob'] < thr,df_c['prob'] >= thr],[0,1])\n",
    "    confusion_matrix = conf_matrix(list(df_c['y']),list(y_hat))\n",
    "    TN = confusion_matrix[0]\n",
    "    FN = confusion_matrix[1]\n",
    "    FP = confusion_matrix[2]\n",
    "    TP = confusion_matrix[3]\n",
    "    A = 500*FN+100*FP\n",
    "    #print(thr,A)\n",
    "    if A < min_A:\n",
    "        best_threshold = thr\n",
    "        min_A = A\n",
    "\n",
    "print(best_threshold,min_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "\n",
    "## D.</b></font> Compute performance metrics(for regression) for the given data 5_d.csv\n",
    "<pre>    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "sVOj-bF9AfCd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d=pd.read_csv('5_d.csv')\n",
    "df_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "uRhL1pheAfCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df_d['y'])\n",
    "y_hat = np.array(df_d['pred'])\n",
    "diff_sqr = np.square(y-y_hat)\n",
    "MSE = np.sum(diff_sqr)/len(y)\n",
    "print(MSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "M_MAPE = np.sum(np.absolute(y-y_hat))/np.sum(y)\n",
    "print(M_MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "y_mean = np.mean(y)\n",
    "diff_sqr = np.square(y-y_mean)\n",
    "ss_total = np.sum(diff_sqr)\n",
    "\n",
    "\n",
    "diff_sqr = np.square(y-y_hat)\n",
    "ss_res = np.sum(diff_sqr)\n",
    "\n",
    "R_sqr = 1 - (ss_res/ss_total)\n",
    "print(R_sqr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "156a3ee15799716d58d55a87e405c1165f4d4c7e5471a2b0ac309addf8a51cbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
